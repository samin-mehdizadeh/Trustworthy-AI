{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYsA-Atnnypb",
        "outputId": "599ed603-e4d9-4f8a-e418-08368f89e476"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision import transforms,datasets, models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from PIL import Image\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = CIFAR10(root='./data', train=True, download=True)\n",
        "testset = CIFAR10(root='./data', train=False, download=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xa1-qp-xn03k",
        "outputId": "c00b518c-5a4a-445a-bc34-74add0c748d4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:13<00:00, 12860488.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "normalize = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "\n",
        "transform = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "        \n",
        "    ])\n",
        "classDict = {'plane': 0, 'car': 1, 'bird': 2, 'cat': 3, 'deer': 4,\n",
        "             'dog': 5, 'frog': 6, 'horse': 7, 'ship': 8, 'truck': 9}"
      ],
      "metadata": {
        "id": "Qw_dhNV7ow6y"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CifarDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self,dataset,idx,transforms=None,exclude_class = 'frog'):\n",
        "        self.idx = idx\n",
        "        self.transforms = transforms\n",
        "        self.data = torch.utils.data.Subset(dataset, idx)\n",
        "        self.exclude_class = exclude_class\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        image,label = self.data[index]\n",
        "        #if(label > classDict[self.exclude_class]):\n",
        "          #label = label-1\n",
        "        if(self.transforms!= None):\n",
        "            image = self.transforms(image)\n",
        "\n",
        "        return image,label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "metadata": {
        "id": "9wT7vnVatEe7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = trainset.data\n",
        "x_test = testset.data\n",
        "y_train = np.array(trainset.targets)\n",
        "y_test = np.array(testset.targets)"
      ],
      "metadata": {
        "id": "RPnvXgE6pYSd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''train_frogs_idx = np.where(y_train == classDict['frog'])[0]\n",
        "test_frogs_idx = np.where(y_test == classDict['frog'])[0]\n",
        "\n",
        "x_train_exclude,x_train_frogs = np.delete(x_train,train_frogs_idx,axis = 0),x_train[train_frogs_idx]\n",
        "x_test_exclude,x_test_frogs = np.delete(x_train,test_frogs_idx,axis = 0),x_test[test_frogs_idx]'''\n",
        "\n",
        "\n",
        "BATCH_SIZE = 256\n",
        "train_dataset = CifarDataset(trainset, np.where(y_train != classDict['frog'])[0], transforms=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,num_workers=2)"
      ],
      "metadata": {
        "id": "7LUWx2AXt2cO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#label = [y for x,y in train_dataset]"
      ],
      "metadata": {
        "id": "ivZPxAox-tNd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#set(label)"
      ],
      "metadata": {
        "id": "_u_eSW6_-8Hh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model,train_dataloader,criterion,optimizer,num_epochs):\n",
        "  for epoch in range(num_epochs): \n",
        "      print(f\"Epoch {epoch+1} running\") \n",
        "      \"\"\" Training Phase \"\"\"\n",
        "      model.train() \n",
        "      running_loss = 0\n",
        "      running_corrects = 0 \n",
        "      total = 0\n",
        "      for i, (inputs, labels) in enumerate(train_dataloader):\n",
        "          inputs = inputs.to(device)\n",
        "          labels = labels.to(device) \n",
        "          optimizer.zero_grad()\n",
        "          outputs = model(inputs)\n",
        "          _, preds = torch.max(outputs, 1)\n",
        "          loss = criterion(outputs, labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          running_loss += loss.item() * inputs.size(0)\n",
        "          running_corrects += torch.sum(preds == labels.data)\n",
        "          total += len(labels)\n",
        "      epoch_loss = running_loss / total\n",
        "      epoch_acc = running_corrects / total * 100.\n",
        "      print('[Train #{}] Loss: {:.4f} Acc: {:.4f}%'.format(epoch+1, epoch_loss, epoch_acc))"
      ],
      "metadata": {
        "id": "WLdRgioDy_n7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model,test_dataloader,criterion,optimizer):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        running_loss = 0.\n",
        "        running_corrects = 0\n",
        "        total = 0\n",
        "        for inputs, labels in test_dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "            total += len(labels)\n",
        "        epoch_loss = running_loss / total\n",
        "        epoch_acc = running_corrects / total * 100.\n",
        "        print('[Test] Loss: {:.4f} Acc: {:.4f}% '.format(epoch_loss, epoch_acc))"
      ],
      "metadata": {
        "id": "ltLhJZBozAoH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_softmax_values(model,test_dataloader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        soft_max = []\n",
        "        total_outliers = 0\n",
        "        total_samples = 0\n",
        "        for inputs, labels in test_dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = model(inputs)\n",
        "            softmax_values = torch.nn.functional.softmax(outputs, dim=1)\n",
        "            for i in range(len(inputs)):\n",
        "              soft_max.append(np.max(softmax_values[i].cpu().detach().numpy()))\n",
        "    return np.array(soft_max)\n",
        "             \n",
        "                "
      ],
      "metadata": {
        "id": "TVN2sJnoDXum"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.resnet18(pretrained = True)   \n",
        "\n",
        "#num_features = model.fc.in_features    \n",
        "#model.fc = nn.Linear(num_features, 9) \n",
        "model = model.to(device) \n",
        "criterion = nn.CrossEntropyLoss() \n",
        "optimizer = optim.Adam(model.parameters())"
      ],
      "metadata": {
        "id": "o4mTBpj5zf0K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5e1a892-c904-4b23-c384-f5670f7a5078"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 174MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "cASsmFMbz8yH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train(model,train_loader,criterion,optimizer,200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-A3SSL-Zz_Sk",
        "outputId": "2813567c-a6b3-4b0c-e6a2-91aea996c127"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 running\n",
            "[Train #1] Loss: 1.2973 Acc: 60.0756%\n",
            "Epoch 2 running\n",
            "[Train #2] Loss: 0.7003 Acc: 75.6600%\n",
            "Epoch 3 running\n",
            "[Train #3] Loss: 0.6028 Acc: 78.9911%\n",
            "Epoch 4 running\n",
            "[Train #4] Loss: 0.5415 Acc: 81.0733%\n",
            "Epoch 5 running\n",
            "[Train #5] Loss: 0.5034 Acc: 82.5822%\n",
            "Epoch 6 running\n",
            "[Train #6] Loss: 0.4699 Acc: 83.5867%\n",
            "Epoch 7 running\n",
            "[Train #7] Loss: 0.4464 Acc: 84.3800%\n",
            "Epoch 8 running\n",
            "[Train #8] Loss: 0.4230 Acc: 85.2133%\n",
            "Epoch 9 running\n",
            "[Train #9] Loss: 0.4029 Acc: 85.7444%\n",
            "Epoch 10 running\n",
            "[Train #10] Loss: 0.3895 Acc: 86.2289%\n",
            "Epoch 11 running\n",
            "[Train #11] Loss: 0.3711 Acc: 86.8844%\n",
            "Epoch 12 running\n",
            "[Train #12] Loss: 0.3602 Acc: 87.2356%\n",
            "Epoch 13 running\n",
            "[Train #13] Loss: 0.3473 Acc: 87.8844%\n",
            "Epoch 14 running\n",
            "[Train #14] Loss: 0.3372 Acc: 88.2222%\n",
            "Epoch 15 running\n",
            "[Train #15] Loss: 0.3236 Acc: 88.6200%\n",
            "Epoch 16 running\n",
            "[Train #16] Loss: 0.3112 Acc: 89.1489%\n",
            "Epoch 17 running\n",
            "[Train #17] Loss: 0.3031 Acc: 89.3044%\n",
            "Epoch 18 running\n",
            "[Train #18] Loss: 0.2894 Acc: 89.5822%\n",
            "Epoch 19 running\n",
            "[Train #19] Loss: 0.2873 Acc: 89.8311%\n",
            "Epoch 20 running\n",
            "[Train #20] Loss: 0.2773 Acc: 90.1844%\n",
            "Epoch 21 running\n",
            "[Train #21] Loss: 0.2697 Acc: 90.4111%\n",
            "Epoch 22 running\n",
            "[Train #22] Loss: 0.2552 Acc: 90.9667%\n",
            "Epoch 23 running\n",
            "[Train #23] Loss: 0.2510 Acc: 91.0889%\n",
            "Epoch 24 running\n",
            "[Train #24] Loss: 0.2439 Acc: 91.4133%\n",
            "Epoch 25 running\n",
            "[Train #25] Loss: 0.2501 Acc: 91.0844%\n",
            "Epoch 26 running\n",
            "[Train #26] Loss: 0.2323 Acc: 91.8622%\n",
            "Epoch 27 running\n",
            "[Train #27] Loss: 0.2230 Acc: 92.0444%\n",
            "Epoch 28 running\n",
            "[Train #28] Loss: 0.2180 Acc: 92.1578%\n",
            "Epoch 29 running\n",
            "[Train #29] Loss: 0.2155 Acc: 92.2156%\n",
            "Epoch 30 running\n",
            "[Train #30] Loss: 0.2107 Acc: 92.5533%\n",
            "Epoch 31 running\n",
            "[Train #31] Loss: 0.2025 Acc: 92.7822%\n",
            "Epoch 32 running\n",
            "[Train #32] Loss: 0.1938 Acc: 93.0222%\n",
            "Epoch 33 running\n",
            "[Train #33] Loss: 0.1919 Acc: 93.0644%\n",
            "Epoch 34 running\n",
            "[Train #34] Loss: 0.1910 Acc: 93.1578%\n",
            "Epoch 35 running\n",
            "[Train #35] Loss: 0.1840 Acc: 93.5133%\n",
            "Epoch 36 running\n",
            "[Train #36] Loss: 0.1730 Acc: 93.8889%\n",
            "Epoch 37 running\n",
            "[Train #37] Loss: 0.1847 Acc: 93.4822%\n",
            "Epoch 38 running\n",
            "[Train #38] Loss: 0.1724 Acc: 93.8067%\n",
            "Epoch 39 running\n",
            "[Train #39] Loss: 0.1647 Acc: 94.0844%\n",
            "Epoch 40 running\n",
            "[Train #40] Loss: 0.1599 Acc: 94.2800%\n",
            "Epoch 41 running\n",
            "[Train #41] Loss: 0.1801 Acc: 93.7222%\n",
            "Epoch 42 running\n",
            "[Train #42] Loss: 0.1624 Acc: 94.3067%\n",
            "Epoch 43 running\n",
            "[Train #43] Loss: 0.1500 Acc: 94.6644%\n",
            "Epoch 44 running\n",
            "[Train #44] Loss: 0.1483 Acc: 94.6644%\n",
            "Epoch 45 running\n",
            "[Train #45] Loss: 0.1430 Acc: 94.8978%\n",
            "Epoch 46 running\n",
            "[Train #46] Loss: 0.1442 Acc: 94.9289%\n",
            "Epoch 47 running\n",
            "[Train #47] Loss: 0.1388 Acc: 94.9822%\n",
            "Epoch 48 running\n",
            "[Train #48] Loss: 0.1316 Acc: 95.3045%\n",
            "Epoch 49 running\n",
            "[Train #49] Loss: 0.1342 Acc: 95.2489%\n",
            "Epoch 50 running\n",
            "[Train #50] Loss: 0.1245 Acc: 95.6378%\n",
            "Epoch 51 running\n",
            "[Train #51] Loss: 0.1259 Acc: 95.5133%\n",
            "Epoch 52 running\n",
            "[Train #52] Loss: 0.1210 Acc: 95.6911%\n",
            "Epoch 53 running\n",
            "[Train #53] Loss: 0.1238 Acc: 95.5422%\n",
            "Epoch 54 running\n",
            "[Train #54] Loss: 0.1233 Acc: 95.6711%\n",
            "Epoch 55 running\n",
            "[Train #55] Loss: 0.1152 Acc: 95.9133%\n",
            "Epoch 56 running\n",
            "[Train #56] Loss: 0.1138 Acc: 95.9244%\n",
            "Epoch 57 running\n",
            "[Train #57] Loss: 0.1144 Acc: 95.9667%\n",
            "Epoch 58 running\n",
            "[Train #58] Loss: 0.1107 Acc: 96.0756%\n",
            "Epoch 59 running\n",
            "[Train #59] Loss: 0.1238 Acc: 95.6489%\n",
            "Epoch 60 running\n",
            "[Train #60] Loss: 0.1070 Acc: 96.1644%\n",
            "Epoch 61 running\n",
            "[Train #61] Loss: 0.1084 Acc: 96.1622%\n",
            "Epoch 62 running\n",
            "[Train #62] Loss: 0.1028 Acc: 96.3511%\n",
            "Epoch 63 running\n",
            "[Train #63] Loss: 0.1031 Acc: 96.3422%\n",
            "Epoch 64 running\n",
            "[Train #64] Loss: 0.1006 Acc: 96.4667%\n",
            "Epoch 65 running\n",
            "[Train #65] Loss: 0.0960 Acc: 96.5733%\n",
            "Epoch 66 running\n",
            "[Train #66] Loss: 0.0974 Acc: 96.6244%\n",
            "Epoch 67 running\n",
            "[Train #67] Loss: 0.1089 Acc: 96.2511%\n",
            "Epoch 68 running\n",
            "[Train #68] Loss: 0.1027 Acc: 96.2756%\n",
            "Epoch 69 running\n",
            "[Train #69] Loss: 0.0937 Acc: 96.7044%\n",
            "Epoch 70 running\n",
            "[Train #70] Loss: 0.0896 Acc: 96.7889%\n",
            "Epoch 71 running\n",
            "[Train #71] Loss: 0.0876 Acc: 96.9378%\n",
            "Epoch 72 running\n",
            "[Train #72] Loss: 0.0833 Acc: 97.0111%\n",
            "Epoch 73 running\n",
            "[Train #73] Loss: 0.0862 Acc: 97.0178%\n",
            "Epoch 74 running\n",
            "[Train #74] Loss: 0.0821 Acc: 97.0778%\n",
            "Epoch 75 running\n",
            "[Train #75] Loss: 0.0776 Acc: 97.2644%\n",
            "Epoch 76 running\n",
            "[Train #76] Loss: 0.0813 Acc: 97.1733%\n",
            "Epoch 77 running\n",
            "[Train #77] Loss: 0.0783 Acc: 97.1978%\n",
            "Epoch 78 running\n",
            "[Train #78] Loss: 0.0803 Acc: 97.1578%\n",
            "Epoch 79 running\n",
            "[Train #79] Loss: 0.0841 Acc: 97.0378%\n",
            "Epoch 80 running\n",
            "[Train #80] Loss: 0.0802 Acc: 97.1667%\n",
            "Epoch 81 running\n",
            "[Train #81] Loss: 0.0747 Acc: 97.2778%\n",
            "Epoch 82 running\n",
            "[Train #82] Loss: 0.0743 Acc: 97.3733%\n",
            "Epoch 83 running\n",
            "[Train #83] Loss: 0.0740 Acc: 97.3711%\n",
            "Epoch 84 running\n",
            "[Train #84] Loss: 0.0774 Acc: 97.3556%\n",
            "Epoch 85 running\n",
            "[Train #85] Loss: 0.0761 Acc: 97.3133%\n",
            "Epoch 86 running\n",
            "[Train #86] Loss: 0.0685 Acc: 97.5578%\n",
            "Epoch 87 running\n",
            "[Train #87] Loss: 0.0727 Acc: 97.4667%\n",
            "Epoch 88 running\n",
            "[Train #88] Loss: 0.0725 Acc: 97.4778%\n",
            "Epoch 89 running\n",
            "[Train #89] Loss: 0.0698 Acc: 97.5022%\n",
            "Epoch 90 running\n",
            "[Train #90] Loss: 0.0648 Acc: 97.7378%\n",
            "Epoch 91 running\n",
            "[Train #91] Loss: 0.0698 Acc: 97.6733%\n",
            "Epoch 92 running\n",
            "[Train #92] Loss: 0.0663 Acc: 97.6844%\n",
            "Epoch 93 running\n",
            "[Train #93] Loss: 0.0646 Acc: 97.8111%\n",
            "Epoch 94 running\n",
            "[Train #94] Loss: 0.0684 Acc: 97.6133%\n",
            "Epoch 95 running\n",
            "[Train #95] Loss: 0.0670 Acc: 97.6622%\n",
            "Epoch 96 running\n",
            "[Train #96] Loss: 0.0611 Acc: 97.8400%\n",
            "Epoch 97 running\n",
            "[Train #97] Loss: 0.0736 Acc: 97.3800%\n",
            "Epoch 98 running\n",
            "[Train #98] Loss: 0.0644 Acc: 97.7844%\n",
            "Epoch 99 running\n",
            "[Train #99] Loss: 0.0726 Acc: 97.5533%\n",
            "Epoch 100 running\n",
            "[Train #100] Loss: 0.0652 Acc: 97.7400%\n",
            "Epoch 101 running\n",
            "[Train #101] Loss: 0.0595 Acc: 97.9111%\n",
            "Epoch 102 running\n",
            "[Train #102] Loss: 0.0952 Acc: 96.7422%\n",
            "Epoch 103 running\n",
            "[Train #103] Loss: 0.0649 Acc: 97.7644%\n",
            "Epoch 104 running\n",
            "[Train #104] Loss: 0.0562 Acc: 98.0222%\n",
            "Epoch 105 running\n",
            "[Train #105] Loss: 0.0548 Acc: 98.0378%\n",
            "Epoch 106 running\n",
            "[Train #106] Loss: 0.0511 Acc: 98.2489%\n",
            "Epoch 107 running\n",
            "[Train #107] Loss: 0.0566 Acc: 98.0111%\n",
            "Epoch 108 running\n",
            "[Train #108] Loss: 0.0610 Acc: 97.9356%\n",
            "Epoch 109 running\n",
            "[Train #109] Loss: 0.0571 Acc: 97.9556%\n",
            "Epoch 110 running\n",
            "[Train #110] Loss: 0.0576 Acc: 97.9844%\n",
            "Epoch 111 running\n",
            "[Train #111] Loss: 0.0537 Acc: 98.0667%\n",
            "Epoch 112 running\n",
            "[Train #112] Loss: 0.0488 Acc: 98.2889%\n",
            "Epoch 113 running\n",
            "[Train #113] Loss: 0.0526 Acc: 98.1689%\n",
            "Epoch 114 running\n",
            "[Train #114] Loss: 0.1171 Acc: 96.2000%\n",
            "Epoch 115 running\n",
            "[Train #115] Loss: 0.0650 Acc: 97.7778%\n",
            "Epoch 116 running\n",
            "[Train #116] Loss: 0.0636 Acc: 97.7689%\n",
            "Epoch 117 running\n",
            "[Train #117] Loss: 0.0537 Acc: 98.0511%\n",
            "Epoch 118 running\n",
            "[Train #118] Loss: 0.0469 Acc: 98.3689%\n",
            "Epoch 119 running\n",
            "[Train #119] Loss: 0.0483 Acc: 98.2444%\n",
            "Epoch 120 running\n",
            "[Train #120] Loss: 0.0434 Acc: 98.4444%\n",
            "Epoch 121 running\n",
            "[Train #121] Loss: 0.0481 Acc: 98.3156%\n",
            "Epoch 122 running\n",
            "[Train #122] Loss: 0.0506 Acc: 98.3044%\n",
            "Epoch 123 running\n",
            "[Train #123] Loss: 0.0452 Acc: 98.4267%\n",
            "Epoch 124 running\n",
            "[Train #124] Loss: 0.0462 Acc: 98.3711%\n",
            "Epoch 125 running\n",
            "[Train #125] Loss: 0.0498 Acc: 98.2667%\n",
            "Epoch 126 running\n",
            "[Train #126] Loss: 0.0425 Acc: 98.5378%\n",
            "Epoch 127 running\n",
            "[Train #127] Loss: 0.0479 Acc: 98.2778%\n",
            "Epoch 128 running\n",
            "[Train #128] Loss: 0.0496 Acc: 98.2978%\n",
            "Epoch 129 running\n",
            "[Train #129] Loss: 0.0428 Acc: 98.5244%\n",
            "Epoch 130 running\n",
            "[Train #130] Loss: 0.0485 Acc: 98.3244%\n",
            "Epoch 131 running\n",
            "[Train #131] Loss: 0.0489 Acc: 98.3000%\n",
            "Epoch 132 running\n",
            "[Train #132] Loss: 0.0440 Acc: 98.4089%\n",
            "Epoch 133 running\n",
            "[Train #133] Loss: 0.0458 Acc: 98.3578%\n",
            "Epoch 134 running\n",
            "[Train #134] Loss: 0.0448 Acc: 98.4333%\n",
            "Epoch 135 running\n",
            "[Train #135] Loss: 0.0468 Acc: 98.3600%\n",
            "Epoch 136 running\n",
            "[Train #136] Loss: 0.0414 Acc: 98.4822%\n",
            "Epoch 137 running\n",
            "[Train #137] Loss: 0.0452 Acc: 98.3889%\n",
            "Epoch 138 running\n",
            "[Train #138] Loss: 0.0427 Acc: 98.5333%\n",
            "Epoch 139 running\n",
            "[Train #139] Loss: 0.0464 Acc: 98.4267%\n",
            "Epoch 140 running\n",
            "[Train #140] Loss: 0.0420 Acc: 98.6133%\n",
            "Epoch 141 running\n",
            "[Train #141] Loss: 0.1991 Acc: 93.7244%\n",
            "Epoch 142 running\n",
            "[Train #142] Loss: 0.1137 Acc: 96.0578%\n",
            "Epoch 143 running\n",
            "[Train #143] Loss: 0.0644 Acc: 97.8622%\n",
            "Epoch 144 running\n",
            "[Train #144] Loss: 0.0485 Acc: 98.3311%\n",
            "Epoch 145 running\n",
            "[Train #145] Loss: 0.0483 Acc: 98.3222%\n",
            "Epoch 146 running\n",
            "[Train #146] Loss: 0.0395 Acc: 98.6111%\n",
            "Epoch 147 running\n",
            "[Train #147] Loss: 0.0335 Acc: 98.8622%\n",
            "Epoch 148 running\n",
            "[Train #148] Loss: 0.0362 Acc: 98.7444%\n",
            "Epoch 149 running\n",
            "[Train #149] Loss: 0.0384 Acc: 98.6733%\n",
            "Epoch 150 running\n",
            "[Train #150] Loss: 0.0337 Acc: 98.8267%\n",
            "Epoch 151 running\n",
            "[Train #151] Loss: 0.0362 Acc: 98.7156%\n",
            "Epoch 152 running\n",
            "[Train #152] Loss: 0.0348 Acc: 98.8022%\n",
            "Epoch 153 running\n",
            "[Train #153] Loss: 0.0353 Acc: 98.7400%\n",
            "Epoch 154 running\n",
            "[Train #154] Loss: 0.0327 Acc: 98.8644%\n",
            "Epoch 155 running\n",
            "[Train #155] Loss: 0.0345 Acc: 98.7978%\n",
            "Epoch 156 running\n",
            "[Train #156] Loss: 0.0354 Acc: 98.7489%\n",
            "Epoch 157 running\n",
            "[Train #157] Loss: 0.0397 Acc: 98.6067%\n",
            "Epoch 158 running\n",
            "[Train #158] Loss: 0.0345 Acc: 98.7822%\n",
            "Epoch 159 running\n",
            "[Train #159] Loss: 0.0379 Acc: 98.7556%\n",
            "Epoch 160 running\n",
            "[Train #160] Loss: 0.0354 Acc: 98.7200%\n",
            "Epoch 161 running\n",
            "[Train #161] Loss: 0.0328 Acc: 98.8289%\n",
            "Epoch 162 running\n",
            "[Train #162] Loss: 0.0354 Acc: 98.7156%\n",
            "Epoch 163 running\n",
            "[Train #163] Loss: 0.0374 Acc: 98.6822%\n",
            "Epoch 164 running\n",
            "[Train #164] Loss: 0.0375 Acc: 98.6378%\n",
            "Epoch 165 running\n",
            "[Train #165] Loss: 0.0408 Acc: 98.6667%\n",
            "Epoch 166 running\n",
            "[Train #166] Loss: 0.0348 Acc: 98.7289%\n",
            "Epoch 167 running\n",
            "[Train #167] Loss: 0.0349 Acc: 98.7911%\n",
            "Epoch 168 running\n",
            "[Train #168] Loss: 0.0386 Acc: 98.6578%\n",
            "Epoch 169 running\n",
            "[Train #169] Loss: 0.0358 Acc: 98.7867%\n",
            "Epoch 170 running\n",
            "[Train #170] Loss: 0.0351 Acc: 98.7867%\n",
            "Epoch 171 running\n",
            "[Train #171] Loss: 0.0380 Acc: 98.6978%\n",
            "Epoch 172 running\n",
            "[Train #172] Loss: 0.0361 Acc: 98.7422%\n",
            "Epoch 173 running\n",
            "[Train #173] Loss: 0.0390 Acc: 98.6400%\n",
            "Epoch 174 running\n",
            "[Train #174] Loss: 0.0372 Acc: 98.7244%\n",
            "Epoch 175 running\n",
            "[Train #175] Loss: 0.0346 Acc: 98.8133%\n",
            "Epoch 176 running\n",
            "[Train #176] Loss: 0.0326 Acc: 98.8711%\n",
            "Epoch 177 running\n",
            "[Train #177] Loss: 0.0430 Acc: 98.4333%\n",
            "Epoch 178 running\n",
            "[Train #178] Loss: 0.0428 Acc: 98.4911%\n",
            "Epoch 179 running\n",
            "[Train #179] Loss: 0.0306 Acc: 98.9467%\n",
            "Epoch 180 running\n",
            "[Train #180] Loss: 0.0316 Acc: 98.9244%\n",
            "Epoch 181 running\n",
            "[Train #181] Loss: 0.0345 Acc: 98.7822%\n",
            "Epoch 182 running\n",
            "[Train #182] Loss: 0.0319 Acc: 98.8911%\n",
            "Epoch 183 running\n",
            "[Train #183] Loss: 0.0308 Acc: 98.8911%\n",
            "Epoch 184 running\n",
            "[Train #184] Loss: 0.0301 Acc: 98.9978%\n",
            "Epoch 185 running\n",
            "[Train #185] Loss: 0.0342 Acc: 98.8133%\n",
            "Epoch 186 running\n",
            "[Train #186] Loss: 0.0331 Acc: 98.8533%\n",
            "Epoch 187 running\n",
            "[Train #187] Loss: 0.0328 Acc: 98.8644%\n",
            "Epoch 188 running\n",
            "[Train #188] Loss: 0.0316 Acc: 98.9222%\n",
            "Epoch 189 running\n",
            "[Train #189] Loss: 0.0310 Acc: 98.9533%\n",
            "Epoch 190 running\n",
            "[Train #190] Loss: 0.0351 Acc: 98.8178%\n",
            "Epoch 191 running\n",
            "[Train #191] Loss: 0.0334 Acc: 98.8222%\n",
            "Epoch 192 running\n",
            "[Train #192] Loss: 0.0352 Acc: 98.7267%\n",
            "Epoch 193 running\n",
            "[Train #193] Loss: 0.0303 Acc: 98.9600%\n",
            "Epoch 194 running\n",
            "[Train #194] Loss: 0.0283 Acc: 98.9822%\n",
            "Epoch 195 running\n",
            "[Train #195] Loss: 0.0318 Acc: 98.9289%\n",
            "Epoch 196 running\n",
            "[Train #196] Loss: 0.0308 Acc: 98.9089%\n",
            "Epoch 197 running\n",
            "[Train #197] Loss: 0.0338 Acc: 98.8267%\n",
            "Epoch 198 running\n",
            "[Train #198] Loss: 0.0314 Acc: 98.9089%\n",
            "Epoch 199 running\n",
            "[Train #199] Loss: 0.0296 Acc: 98.9422%\n",
            "Epoch 200 running\n",
            "[Train #200] Loss: 0.0302 Acc: 98.9578%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(),'non_frog.pth')"
      ],
      "metadata": {
        "id": "rXwLgn5rIzdx"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.resnet18(pretrained = True)   \n",
        "#num_features = model.fc.in_features    \n",
        "#model.fc = nn.Linear(num_features, 9) \n",
        "model = model.to(device) \n",
        "criterion = nn.CrossEntropyLoss() \n",
        "optimizer = optim.Adam(model.parameters())\n",
        "model.load_state_dict(torch.load('non_frog.pth'))"
      ],
      "metadata": {
        "id": "uO6-Ykl_Yc-0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45d5343d-2601-47db-e5ca-38afd841ed0b"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "QEM62giDa9QA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ttransform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ])\n",
        "\n",
        "BATCH_SIZE = 256\n",
        "non_frog_dataset = CifarDataset(testset, np.where(y_test != classDict['frog'])[0], transforms=ttransform)\n",
        "non_frog_loader = DataLoader(non_frog_dataset, batch_size=BATCH_SIZE, shuffle=False,num_workers=2)\n",
        "\n",
        "frog_dataset = CifarDataset(testset, np.where(y_test == classDict['frog'])[0], transforms=ttransform)\n",
        "frog_loader = DataLoader(frog_dataset, batch_size=BATCH_SIZE, shuffle=False,num_workers=2)"
      ],
      "metadata": {
        "id": "Sk2OZeX3a_7A"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test(model,non_frog_loader,criterion,optimizer)"
      ],
      "metadata": {
        "id": "gEAJktILa0KA",
        "outputId": "fca718b2-5e2b-4fb6-95c3-7976e66fe25b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Test] Loss: 0.7650 Acc: 86.4444% \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OOD Detection frog"
      ],
      "metadata": {
        "id": "B2RLWioweEzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "soft_max = get_softmax_values(model,non_frog_loader)\n",
        "cutoff = np.percentile(soft_max, 5)\n",
        "inlier = (soft_max > cutoff).sum() / len(soft_max)\n",
        "print(f'non frog data => threshold: {cutoff}, inlier percentage: {inlier*100}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "du0CwNDyWsLQ",
        "outputId": "f48484b9-688d-444a-bf2c-c15d52e65af6"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "non frog data => threshold: 0.7182219207286835, inlier percentage: 95.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "soft_max = get_softmax_values(model,frog_loader)\n",
        "outlier = (soft_max < cutoff).sum() / len(soft_max)\n",
        "print(f'frog data => threshold: {cutoff}, outlier percentage: {outlier*100}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyEsGBOZXzMX",
        "outputId": "914bfd5e-cfe1-4652-c212-9d3381836f36"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "frog data => threshold: 0.7182219207286835, outlier percentage: 17.2%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OOD Detection cat"
      ],
      "metadata": {
        "id": "wRKMiwn0hPja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 256\n",
        "train_dataset = CifarDataset(trainset, np.where(y_train != classDict['cat'])[0], transforms=transform,exclude_class = 'cat')\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,num_workers=2)"
      ],
      "metadata": {
        "id": "h4rpf8mWhQzI"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.resnet18(pretrained = True)   \n",
        "#num_features = model.fc.in_features    \n",
        "#model.fc = nn.Linear(num_features, 9) \n",
        "model = model.to(device) \n",
        "criterion = nn.CrossEntropyLoss() \n",
        "optimizer = optim.Adam(model.parameters())"
      ],
      "metadata": {
        "id": "o95P1XX5hZ0D"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model,train_loader,criterion,optimizer,200)"
      ],
      "metadata": {
        "id": "M3Eh0DLAhgql",
        "outputId": "353b9680-6efb-447d-cc97-5571a53658bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 running\n",
            "[Train #1] Loss: 1.2607 Acc: 62.3133%\n",
            "Epoch 2 running\n",
            "[Train #2] Loss: 0.6166 Acc: 78.9644%\n",
            "Epoch 3 running\n",
            "[Train #3] Loss: 0.5162 Acc: 82.3000%\n",
            "Epoch 4 running\n",
            "[Train #4] Loss: 0.4689 Acc: 84.0956%\n",
            "Epoch 5 running\n",
            "[Train #5] Loss: 0.4272 Acc: 85.3867%\n",
            "Epoch 6 running\n",
            "[Train #6] Loss: 0.3969 Acc: 86.3956%\n",
            "Epoch 7 running\n",
            "[Train #7] Loss: 0.3749 Acc: 87.0222%\n",
            "Epoch 8 running\n",
            "[Train #8] Loss: 0.3489 Acc: 88.1356%\n",
            "Epoch 9 running\n",
            "[Train #9] Loss: 0.3352 Acc: 88.5444%\n",
            "Epoch 10 running\n",
            "[Train #10] Loss: 0.3167 Acc: 89.2133%\n",
            "Epoch 11 running\n",
            "[Train #11] Loss: 0.3057 Acc: 89.5956%\n",
            "Epoch 12 running\n",
            "[Train #12] Loss: 0.2933 Acc: 89.9978%\n",
            "Epoch 13 running\n",
            "[Train #13] Loss: 0.2829 Acc: 90.3644%\n",
            "Epoch 14 running\n",
            "[Train #14] Loss: 0.2773 Acc: 90.4600%\n",
            "Epoch 15 running\n",
            "[Train #15] Loss: 0.2597 Acc: 91.0889%\n",
            "Epoch 16 running\n",
            "[Train #16] Loss: 0.2462 Acc: 91.3889%\n",
            "Epoch 17 running\n",
            "[Train #17] Loss: 0.2425 Acc: 91.6711%\n",
            "Epoch 18 running\n",
            "[Train #18] Loss: 0.2352 Acc: 91.9689%\n",
            "Epoch 19 running\n",
            "[Train #19] Loss: 0.2240 Acc: 92.2489%\n",
            "Epoch 20 running\n",
            "[Train #20] Loss: 0.2208 Acc: 92.2356%\n",
            "Epoch 21 running\n",
            "[Train #21] Loss: 0.2132 Acc: 92.5511%\n",
            "Epoch 22 running\n",
            "[Train #22] Loss: 0.2026 Acc: 92.9267%\n",
            "Epoch 23 running\n",
            "[Train #23] Loss: 0.1977 Acc: 93.1400%\n",
            "Epoch 24 running\n",
            "[Train #24] Loss: 0.2121 Acc: 92.6267%\n",
            "Epoch 25 running\n",
            "[Train #25] Loss: 0.2063 Acc: 92.9000%\n",
            "Epoch 26 running\n",
            "[Train #26] Loss: 0.1811 Acc: 93.6222%\n",
            "Epoch 27 running\n",
            "[Train #27] Loss: 0.1796 Acc: 93.7733%\n",
            "Epoch 28 running\n",
            "[Train #28] Loss: 0.1696 Acc: 94.0667%\n",
            "Epoch 29 running\n",
            "[Train #29] Loss: 0.1647 Acc: 94.2111%\n",
            "Epoch 30 running\n",
            "[Train #30] Loss: 0.1624 Acc: 94.3467%\n",
            "Epoch 31 running\n",
            "[Train #31] Loss: 0.1549 Acc: 94.6044%\n",
            "Epoch 32 running\n",
            "[Train #32] Loss: 0.1494 Acc: 94.7244%\n",
            "Epoch 33 running\n",
            "[Train #33] Loss: 0.1486 Acc: 94.8667%\n",
            "Epoch 34 running\n",
            "[Train #34] Loss: 0.1461 Acc: 94.9089%\n",
            "Epoch 35 running\n",
            "[Train #35] Loss: 0.1416 Acc: 94.9911%\n",
            "Epoch 36 running\n",
            "[Train #36] Loss: 0.1426 Acc: 94.9267%\n",
            "Epoch 37 running\n",
            "[Train #37] Loss: 0.1402 Acc: 95.0489%\n",
            "Epoch 38 running\n",
            "[Train #38] Loss: 0.1348 Acc: 95.2800%\n",
            "Epoch 39 running\n",
            "[Train #39] Loss: 0.1275 Acc: 95.5111%\n",
            "Epoch 40 running\n",
            "[Train #40] Loss: 0.1254 Acc: 95.5178%\n",
            "Epoch 41 running\n",
            "[Train #41] Loss: 0.1259 Acc: 95.5489%\n",
            "Epoch 42 running\n",
            "[Train #42] Loss: 0.1226 Acc: 95.7444%\n",
            "Epoch 43 running\n",
            "[Train #43] Loss: 0.1607 Acc: 94.5111%\n",
            "Epoch 44 running\n",
            "[Train #44] Loss: 0.1196 Acc: 95.8756%\n",
            "Epoch 45 running\n",
            "[Train #45] Loss: 0.1081 Acc: 96.2222%\n",
            "Epoch 46 running\n",
            "[Train #46] Loss: 0.1071 Acc: 96.2933%\n",
            "Epoch 47 running\n",
            "[Train #47] Loss: 0.1061 Acc: 96.2400%\n",
            "Epoch 48 running\n",
            "[Train #48] Loss: 0.1019 Acc: 96.3778%\n",
            "Epoch 49 running\n",
            "[Train #49] Loss: 0.1091 Acc: 96.2089%\n",
            "Epoch 50 running\n",
            "[Train #50] Loss: 0.0997 Acc: 96.5022%\n",
            "Epoch 51 running\n",
            "[Train #51] Loss: 0.0987 Acc: 96.5356%\n",
            "Epoch 52 running\n",
            "[Train #52] Loss: 0.1004 Acc: 96.4956%\n",
            "Epoch 53 running\n",
            "[Train #53] Loss: 0.0922 Acc: 96.7911%\n",
            "Epoch 54 running\n",
            "[Train #54] Loss: 0.0910 Acc: 96.7956%\n",
            "Epoch 55 running\n",
            "[Train #55] Loss: 0.0910 Acc: 96.7400%\n",
            "Epoch 56 running\n",
            "[Train #56] Loss: 0.0866 Acc: 96.9356%\n",
            "Epoch 57 running\n",
            "[Train #57] Loss: 0.0860 Acc: 96.9044%\n",
            "Epoch 58 running\n",
            "[Train #58] Loss: 0.1245 Acc: 95.7400%\n",
            "Epoch 59 running\n",
            "[Train #59] Loss: 0.0890 Acc: 96.8844%\n",
            "Epoch 60 running\n",
            "[Train #60] Loss: 0.0877 Acc: 96.9733%\n",
            "Epoch 61 running\n",
            "[Train #61] Loss: 0.0821 Acc: 97.1444%\n",
            "Epoch 62 running\n",
            "[Train #62] Loss: 0.0817 Acc: 97.1867%\n",
            "Epoch 63 running\n",
            "[Train #63] Loss: 0.0725 Acc: 97.4889%\n",
            "Epoch 64 running\n",
            "[Train #64] Loss: 0.0791 Acc: 97.1667%\n",
            "Epoch 65 running\n",
            "[Train #65] Loss: 0.0801 Acc: 97.2133%\n",
            "Epoch 66 running\n",
            "[Train #66] Loss: 0.0731 Acc: 97.4422%\n",
            "Epoch 67 running\n",
            "[Train #67] Loss: 0.0724 Acc: 97.4444%\n",
            "Epoch 68 running\n",
            "[Train #68] Loss: 0.0743 Acc: 97.4778%\n",
            "Epoch 69 running\n",
            "[Train #69] Loss: 0.0730 Acc: 97.4356%\n",
            "Epoch 70 running\n",
            "[Train #70] Loss: 0.0791 Acc: 97.2467%\n",
            "Epoch 71 running\n",
            "[Train #71] Loss: 0.0872 Acc: 96.9511%\n",
            "Epoch 72 running\n",
            "[Train #72] Loss: 0.0710 Acc: 97.5489%\n",
            "Epoch 73 running\n",
            "[Train #73] Loss: 0.0647 Acc: 97.7533%\n",
            "Epoch 74 running\n",
            "[Train #74] Loss: 0.0676 Acc: 97.6244%\n",
            "Epoch 75 running\n",
            "[Train #75] Loss: 0.0640 Acc: 97.7156%\n",
            "Epoch 76 running\n",
            "[Train #76] Loss: 0.0678 Acc: 97.5578%\n",
            "Epoch 77 running\n",
            "[Train #77] Loss: 0.0630 Acc: 97.7600%\n",
            "Epoch 78 running\n",
            "[Train #78] Loss: 0.0651 Acc: 97.6600%\n",
            "Epoch 79 running\n",
            "[Train #79] Loss: 0.0621 Acc: 97.8022%\n",
            "Epoch 80 running\n",
            "[Train #80] Loss: 0.0615 Acc: 97.8644%\n",
            "Epoch 81 running\n",
            "[Train #81] Loss: 0.0593 Acc: 97.9422%\n",
            "Epoch 82 running\n",
            "[Train #82] Loss: 0.0578 Acc: 98.0333%\n",
            "Epoch 83 running\n",
            "[Train #83] Loss: 0.0601 Acc: 97.8533%\n",
            "Epoch 84 running\n",
            "[Train #84] Loss: 0.0607 Acc: 97.8933%\n",
            "Epoch 85 running\n",
            "[Train #85] Loss: 0.0560 Acc: 98.0378%\n",
            "Epoch 86 running\n",
            "[Train #86] Loss: 0.0598 Acc: 97.9178%\n",
            "Epoch 87 running\n",
            "[Train #87] Loss: 0.0565 Acc: 98.1133%\n",
            "Epoch 88 running\n",
            "[Train #88] Loss: 0.0985 Acc: 96.7511%\n",
            "Epoch 89 running\n",
            "[Train #89] Loss: 0.0843 Acc: 97.0689%\n",
            "Epoch 90 running\n",
            "[Train #90] Loss: 0.0659 Acc: 97.6911%\n",
            "Epoch 91 running\n",
            "[Train #91] Loss: 0.0538 Acc: 98.1111%\n",
            "Epoch 92 running\n",
            "[Train #92] Loss: 0.0534 Acc: 98.1022%\n",
            "Epoch 93 running\n",
            "[Train #93] Loss: 0.0484 Acc: 98.2711%\n",
            "Epoch 94 running\n",
            "[Train #94] Loss: 0.0869 Acc: 97.1289%\n",
            "Epoch 95 running\n",
            "[Train #95] Loss: 0.0538 Acc: 98.2111%\n",
            "Epoch 96 running\n",
            "[Train #96] Loss: 0.0477 Acc: 98.3467%\n",
            "Epoch 97 running\n",
            "[Train #97] Loss: 0.0502 Acc: 98.2289%\n",
            "Epoch 98 running\n",
            "[Train #98] Loss: 0.0602 Acc: 97.9089%\n",
            "Epoch 99 running\n",
            "[Train #99] Loss: 0.0493 Acc: 98.3022%\n",
            "Epoch 100 running\n",
            "[Train #100] Loss: 0.0429 Acc: 98.4800%\n",
            "Epoch 101 running\n",
            "[Train #101] Loss: 0.0465 Acc: 98.4067%\n",
            "Epoch 102 running\n",
            "[Train #102] Loss: 0.0457 Acc: 98.3644%\n",
            "Epoch 103 running\n",
            "[Train #103] Loss: 0.0473 Acc: 98.4444%\n",
            "Epoch 104 running\n",
            "[Train #104] Loss: 0.0468 Acc: 98.4000%\n",
            "Epoch 105 running\n",
            "[Train #105] Loss: 0.0450 Acc: 98.4622%\n",
            "Epoch 106 running\n",
            "[Train #106] Loss: 0.0686 Acc: 97.5933%\n",
            "Epoch 107 running\n",
            "[Train #107] Loss: 0.0482 Acc: 98.3422%\n",
            "Epoch 108 running\n",
            "[Train #108] Loss: 0.0409 Acc: 98.5533%\n",
            "Epoch 109 running\n",
            "[Train #109] Loss: 0.0471 Acc: 98.4178%\n",
            "Epoch 110 running\n",
            "[Train #110] Loss: 0.0426 Acc: 98.5267%\n",
            "Epoch 111 running\n",
            "[Train #111] Loss: 0.0463 Acc: 98.3400%\n",
            "Epoch 112 running\n",
            "[Train #112] Loss: 0.0446 Acc: 98.4289%\n",
            "Epoch 113 running\n",
            "[Train #113] Loss: 0.0468 Acc: 98.3333%\n",
            "Epoch 114 running\n",
            "[Train #114] Loss: 0.0452 Acc: 98.4289%\n",
            "Epoch 115 running\n",
            "[Train #115] Loss: 0.0415 Acc: 98.6000%\n",
            "Epoch 116 running\n",
            "[Train #116] Loss: 0.0387 Acc: 98.7222%\n",
            "Epoch 117 running\n",
            "[Train #117] Loss: 0.0391 Acc: 98.6378%\n",
            "Epoch 118 running\n",
            "[Train #118] Loss: 0.0409 Acc: 98.5511%\n",
            "Epoch 119 running\n",
            "[Train #119] Loss: 0.0433 Acc: 98.5400%\n",
            "Epoch 120 running\n",
            "[Train #120] Loss: 0.0436 Acc: 98.4667%\n",
            "Epoch 121 running\n",
            "[Train #121] Loss: 0.0391 Acc: 98.5867%\n",
            "Epoch 122 running\n",
            "[Train #122] Loss: 0.0424 Acc: 98.5178%\n",
            "Epoch 123 running\n",
            "[Train #123] Loss: 0.0426 Acc: 98.5267%\n",
            "Epoch 124 running\n",
            "[Train #124] Loss: 0.0402 Acc: 98.5844%\n",
            "Epoch 125 running\n",
            "[Train #125] Loss: 0.0371 Acc: 98.7689%\n",
            "Epoch 126 running\n",
            "[Train #126] Loss: 0.0400 Acc: 98.6267%\n",
            "Epoch 127 running\n",
            "[Train #127] Loss: 0.0367 Acc: 98.7156%\n",
            "Epoch 128 running\n",
            "[Train #128] Loss: 0.0812 Acc: 97.4089%\n",
            "Epoch 129 running\n",
            "[Train #129] Loss: 0.0650 Acc: 97.7000%\n",
            "Epoch 130 running\n",
            "[Train #130] Loss: 0.0419 Acc: 98.6000%\n",
            "Epoch 131 running\n",
            "[Train #131] Loss: 0.0412 Acc: 98.5422%\n",
            "Epoch 132 running\n",
            "[Train #132] Loss: 0.0342 Acc: 98.8133%\n",
            "Epoch 133 running\n",
            "[Train #133] Loss: 0.0318 Acc: 98.9111%\n",
            "Epoch 134 running\n",
            "[Train #134] Loss: 0.0360 Acc: 98.7600%\n",
            "Epoch 135 running\n",
            "[Train #135] Loss: 0.0333 Acc: 98.8578%\n",
            "Epoch 136 running\n",
            "[Train #136] Loss: 0.0319 Acc: 98.8756%\n",
            "Epoch 137 running\n",
            "[Train #137] Loss: 0.0342 Acc: 98.8044%\n",
            "Epoch 138 running\n",
            "[Train #138] Loss: 0.0378 Acc: 98.6978%\n",
            "Epoch 139 running\n",
            "[Train #139] Loss: 0.0362 Acc: 98.7133%\n",
            "Epoch 140 running\n",
            "[Train #140] Loss: 0.0314 Acc: 98.8711%\n",
            "Epoch 141 running\n",
            "[Train #141] Loss: 0.0342 Acc: 98.7911%\n",
            "Epoch 142 running\n",
            "[Train #142] Loss: 0.0344 Acc: 98.7844%\n",
            "Epoch 143 running\n",
            "[Train #143] Loss: 0.0341 Acc: 98.8556%\n",
            "Epoch 144 running\n",
            "[Train #144] Loss: 0.0391 Acc: 98.6800%\n",
            "Epoch 145 running\n",
            "[Train #145] Loss: 0.0932 Acc: 96.9733%\n",
            "Epoch 146 running\n",
            "[Train #146] Loss: 0.0408 Acc: 98.6067%\n",
            "Epoch 147 running\n",
            "[Train #147] Loss: 0.0307 Acc: 98.9600%\n",
            "Epoch 148 running\n",
            "[Train #148] Loss: 0.0271 Acc: 99.0733%\n",
            "Epoch 149 running\n",
            "[Train #149] Loss: 0.0313 Acc: 98.8956%\n",
            "Epoch 150 running\n",
            "[Train #150] Loss: 0.0274 Acc: 99.0689%\n",
            "Epoch 151 running\n",
            "[Train #151] Loss: 0.0309 Acc: 98.9600%\n",
            "Epoch 152 running\n",
            "[Train #152] Loss: 0.0313 Acc: 98.9333%\n",
            "Epoch 153 running\n",
            "[Train #153] Loss: 0.0292 Acc: 99.0067%\n",
            "Epoch 154 running\n",
            "[Train #154] Loss: 0.0317 Acc: 98.9089%\n",
            "Epoch 155 running\n",
            "[Train #155] Loss: 0.0343 Acc: 98.7800%\n",
            "Epoch 156 running\n",
            "[Train #156] Loss: 0.0281 Acc: 98.9956%\n",
            "Epoch 157 running\n",
            "[Train #157] Loss: 0.0294 Acc: 98.9689%\n",
            "Epoch 158 running\n",
            "[Train #158] Loss: 0.0327 Acc: 98.8800%\n",
            "Epoch 159 running\n",
            "[Train #159] Loss: 0.0319 Acc: 98.9378%\n",
            "Epoch 160 running\n",
            "[Train #160] Loss: 0.0289 Acc: 99.0111%\n",
            "Epoch 161 running\n",
            "[Train #161] Loss: 0.0331 Acc: 98.8756%\n",
            "Epoch 162 running\n",
            "[Train #162] Loss: 0.0263 Acc: 99.0822%\n",
            "Epoch 163 running\n",
            "[Train #163] Loss: 0.0323 Acc: 98.8600%\n",
            "Epoch 164 running\n",
            "[Train #164] Loss: 0.0292 Acc: 99.0133%\n",
            "Epoch 165 running\n",
            "[Train #165] Loss: 0.0296 Acc: 98.9867%\n",
            "Epoch 166 running\n",
            "[Train #166] Loss: 0.0337 Acc: 98.8311%\n",
            "Epoch 167 running\n",
            "[Train #167] Loss: 0.0289 Acc: 99.0756%\n",
            "Epoch 168 running\n",
            "[Train #168] Loss: 0.0341 Acc: 98.8289%\n",
            "Epoch 169 running\n",
            "[Train #169] Loss: 0.0324 Acc: 98.9556%\n",
            "Epoch 170 running\n",
            "[Train #170] Loss: 0.0282 Acc: 99.0156%\n",
            "Epoch 171 running\n",
            "[Train #171] Loss: 0.0279 Acc: 99.0489%\n",
            "Epoch 172 running\n",
            "[Train #172] Loss: 0.0308 Acc: 98.9333%\n",
            "Epoch 173 running\n",
            "[Train #173] Loss: 0.0284 Acc: 99.0111%\n",
            "Epoch 174 running\n",
            "[Train #174] Loss: 0.0287 Acc: 98.9444%\n",
            "Epoch 175 running\n",
            "[Train #175] Loss: 0.0254 Acc: 99.0889%\n",
            "Epoch 176 running\n",
            "[Train #176] Loss: 0.0331 Acc: 98.8600%\n",
            "Epoch 177 running\n",
            "[Train #177] Loss: 0.0277 Acc: 99.0289%\n",
            "Epoch 178 running\n",
            "[Train #178] Loss: 0.0289 Acc: 99.0289%\n",
            "Epoch 179 running\n",
            "[Train #179] Loss: 0.0258 Acc: 99.0711%\n",
            "Epoch 180 running\n",
            "[Train #180] Loss: 0.0513 Acc: 98.5311%\n",
            "Epoch 181 running\n",
            "[Train #181] Loss: 0.0470 Acc: 98.4333%\n",
            "Epoch 182 running\n",
            "[Train #182] Loss: 0.0316 Acc: 98.8756%\n",
            "Epoch 183 running\n",
            "[Train #183] Loss: 0.0260 Acc: 99.1089%\n",
            "Epoch 184 running\n",
            "[Train #184] Loss: 0.0207 Acc: 99.2644%\n",
            "Epoch 185 running\n",
            "[Train #185] Loss: 0.0240 Acc: 99.1244%\n",
            "Epoch 186 running\n",
            "[Train #186] Loss: 0.0254 Acc: 99.1067%\n",
            "Epoch 187 running\n",
            "[Train #187] Loss: 0.0262 Acc: 99.0644%\n",
            "Epoch 188 running\n",
            "[Train #188] Loss: 0.0285 Acc: 98.9889%\n",
            "Epoch 189 running\n",
            "[Train #189] Loss: 0.0651 Acc: 97.8956%\n",
            "Epoch 190 running\n",
            "[Train #190] Loss: 0.0284 Acc: 99.0422%\n",
            "Epoch 191 running\n",
            "[Train #191] Loss: 0.0316 Acc: 98.9822%\n",
            "Epoch 192 running\n",
            "[Train #192] Loss: 0.0274 Acc: 99.0978%\n",
            "Epoch 193 running\n",
            "[Train #193] Loss: 0.0246 Acc: 99.1533%\n",
            "Epoch 194 running\n",
            "[Train #194] Loss: 0.0227 Acc: 99.1889%\n",
            "Epoch 195 running\n",
            "[Train #195] Loss: 0.0264 Acc: 99.0956%\n",
            "Epoch 196 running\n",
            "[Train #196] Loss: 0.0229 Acc: 99.2089%\n",
            "Epoch 197 running\n",
            "[Train #197] Loss: 0.0245 Acc: 99.1800%\n",
            "Epoch 198 running\n",
            "[Train #198] Loss: 0.0238 Acc: 99.1644%\n",
            "Epoch 199 running\n",
            "[Train #199] Loss: 0.0247 Acc: 99.1133%\n",
            "Epoch 200 running\n",
            "[Train #200] Loss: 0.0257 Acc: 99.1356%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(),'non_cat.pth')"
      ],
      "metadata": {
        "id": "-KUQ3zZ15mnF"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.resnet18(pretrained = True)   \n",
        "#num_features = model.fc.in_features    \n",
        "#model.fc = nn.Linear(num_features, 9) \n",
        "model = model.to(device) \n",
        "criterion = nn.CrossEntropyLoss() \n",
        "optimizer = optim.Adam(model.parameters())\n",
        "model.load_state_dict(torch.load('non_cat.pth'))"
      ],
      "metadata": {
        "id": "GJ7FyBlWyRUR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3ba6921-7b54-42c6-a180-5d3a3f670d03"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 256\n",
        "non_cat_dataset = CifarDataset(testset, np.where(y_test != classDict['cat'])[0], transforms=ttransform,exclude_class = 'cat')\n",
        "non_cat_loader = DataLoader(non_cat_dataset, batch_size=BATCH_SIZE, shuffle=False,num_workers=2)\n",
        "\n",
        "cat_dataset = CifarDataset(testset, np.where(y_test == classDict['cat'])[0], transforms=ttransform,exclude_class = 'cat')\n",
        "cat_loader = DataLoader(cat_dataset, batch_size=BATCH_SIZE, shuffle=False,num_workers=2)"
      ],
      "metadata": {
        "id": "rMQvFFs6hk8B"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test(model,non_cat_loader,criterion,optimizer)"
      ],
      "metadata": {
        "id": "vrrPAZDmhnHo",
        "outputId": "d54c31ec-005b-443d-fbca-2b67d95e2a74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Test] Loss: 0.5636 Acc: 89.6444% \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "soft_max = get_softmax_values(model,non_cat_loader)\n",
        "cutoff = np.percentile(soft_max, 5)"
      ],
      "metadata": {
        "id": "9wRb_7Qk560P"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inlier = (soft_max > cutoff).sum() / len(soft_max)\n",
        "print(f'non cat data => threshold: {cutoff}, inlier percentage: {inlier*100}%')"
      ],
      "metadata": {
        "id": "LMFHzQvb595K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1552b5d7-be0e-4295-a0b7-e616eeafc11b"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "non cat data => threshold: 0.7593916416168213, inlier percentage: 95.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "soft_max = get_softmax_values(model,cat_loader)\n",
        "outlier = (soft_max < cutoff).sum() / len(soft_max)\n",
        "print(f'cat data => threshold: {cutoff}, outlier percentage: {outlier*100}%')"
      ],
      "metadata": {
        "id": "UT-cKhin5-eX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad2dae73-d80f-45de-cdf4-3a53bc49587b"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat data => threshold: 0.7593916416168213, outlier percentage: 20.200000000000003%\n"
          ]
        }
      ]
    }
  ]
}